import torchimport torchvisionfrom torch import nnfrom torch.autograd import Variablefrom torch.utils.data import DataLoaderfrom torchvision import transformsfrom torchvision.utils import save_imageimport torch.utils.data as Datafrom torchvision.datasets import MNISTimport osimport utilimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltuse_cuda = torch.cuda.is_available()device = torch.device('cuda:0' if use_cuda else 'cpu')if not os.path.exists('./dc_img'):    os.mkdir('./dc_img')def to_img(x):    x = 0.5 * (x + 1)    x = x.clamp(0, 1)    x = x.view(x.size(0), 1, 200, 200)    return xnum_epochs = 100000batch_size = 128learning_rate = 1e-3# img_transform = transforms.Compose([#     transforms.ToTensor(),#     # transforms.Normalize(0.5, 0.5)# ])# dataset = MNIST('./data', transform=img_transform, download=True)# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=class FaceLandmarksDataset(Data.Dataset):    """Face Landmarks dataset."""    def __init__(self, root_dir, csv_file, transform=None):        """        Args:            csv_file (string): Path to the csv file with annotations.            root_dir (string): Directory with all the images.            transform (callable, optional): Optional transform to be applied                on a sample.        """        util.read_bins_toCSV(root_dir, csv_file, 200, 200, True)        self.landmarks_frame = pd.read_csv(csv_file)        self.root_dir = root_dir        self.transform = transform    def __len__(self):        return len(self.landmarks_frame)    def __getitem__(self, idx):        if torch.is_tensor(idx):            idx = idx.tolist()        image = util.read_bin(self.landmarks_frame.iloc[idx, 0], (200, 200), True)        bk = util.read_bin(self.landmarks_frame.iloc[idx, 1], (200, 200), True)        ipp = util.read_8bit_bin(self.landmarks_frame.iloc[idx, 2], (200, 200), True).astype(np.int)        diff = util.subtract(image, bk)#.astype(np.int)        diff = ((diff - np.min(diff)) * 255 / np.ptp(diff)).astype(np.int)        if self.transform:            diff, ipp = self.transform(diff, ipp)        return diff, ippdata_transform = transforms.Compose([        transforms.ToPILImage(),        transforms.RandomHorizontalFlip(),        transforms.RandomVerticalFlip(),        transforms.ToTensor(),        # transforms.Normalize(mean = (0.5,), std = (0.5,))    ])dataset = FaceLandmarksDataset(root_dir='D:\\data\\a71\\2PB\\0229\\0229_A71_5_P_48.1x1_(100_99)\\17110602',                                    csv_file='list.csv', transform=data_transform)dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)# #read data# img_list, bk_list, ipp_list, bds_list = util.read_bins("D:\\data\\a71\\2PB\\0229\\0229_A71_5_P_48.1x1_(100_99)\\17110602", 200, 200, True)## diff_norm_list = []# ipp_norm_list = []# for i in range(len(img_list)):#     diff = util.substract(img_list[i], bk_list[i])#     norm = (diff - np.mean(diff)) / np.std(diff)#     norm1 = (ipp_list[i] - np.mean(ipp_list[i]))/np.std(ipp_list[i])#     diff_norm_list.append((diff - np.mean(diff))/np.std(diff))#     ipp_norm_list.append((ipp_list[i] - np.mean(ipp_list[i]))/np.std(ipp_list[i]))## # Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)# torch_dataset = Data.TensorDataset(torch.FloatTensor(diff_norm_list), torch.FloatTensor(ipp_norm_list))# dataloader = Data.DataLoader(dataset=torch_dataset, batch_size=batch_size, shuffle=True)class autoencoder(nn.Module):    def __init__(self):        super(autoencoder, self).__init__()        # self.encoder = nn.Sequential(        #     nn.Conv2d(1, 16, 3, stride=2, padding=0),  # b, 16, 10, 10        #     nn.ReLU(True),        #     nn.MaxPool2d(2),  # b, 16, 5, 5        #     nn.Conv2d(16, 8, 3, stride=2, padding=0),  # b, 8, 3, 3        #     nn.ReLU(True),        #     nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2        # )        # self.decoder = nn.Sequential(        #     nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5        #     nn.ReLU(True),        #     nn.ConvTranspose2d(16, 8, 5, stride=2, padding=0),  # b, 8, 15, 15        #     nn.ReLU(True),        #     nn.ConvTranspose2d(8, 1, 2, stride=2, padding=0),  # b, 1, 28, 28        #     nn.Tanh()        # )        self.encoder = nn.Sequential(         # input shape (1, 200, 200)            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=64, out_channels=200, kernel_size=5, stride=1, padding=2),            nn.ReLU(),        )        self.decoder = nn.Sequential(            nn.Conv2d(in_channels=200, out_channels=64, kernel_size=5, stride=1, padding=2),            nn.ReLU(),            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ReLU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),            nn.ReLU(),        )    def forward(self, x):        x = self.encoder(x)        x = self.decoder(x)        return xmodel = autoencoder().cuda()criterion = nn.MSELoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,                             weight_decay=1e-5)N_TEST_IMG = 5f, a = plt.subplots(3, N_TEST_IMG, figsize=(10, 6))plt.ion()   # continuously plotfor epoch in range(num_epochs):    for data in dataloader:        img, label = data        img = Variable(img).to(device=device, dtype=torch.float)        label = Variable(label).to(device=device, dtype=torch.float)        # ===================forward=====================        output = model(img)        loss = criterion(output, label)        # ===================backward====================        optimizer.zero_grad()        loss.backward()        optimizer.step()    # ===================log========================    print('epoch [{}/{}], loss:{:.4f}'          .format(epoch+1, num_epochs, loss.data))    if epoch % 100 == 0:        # plotting decoded image        for i in range(N_TEST_IMG):            a[0][i].clear()            a[0][i].imshow(np.reshape(img.data.cpu().numpy()[i], (200, 200)), cmap='gray')            a[1][i].clear()            a[1][i].imshow(np.reshape(label.data.cpu().numpy()[i], (200, 200)), cmap='gray')            a[2][i].clear()            a[2][i].imshow(np.reshape(output.data.cpu().numpy()[i], (200, 200)), cmap='gray')        plt.draw()        plt.pause(0.05)        plt.savefig('./dc_img/image_{}.png'.format(epoch))torch.save(model.state_dict(), './conv_autoencoder.pth')