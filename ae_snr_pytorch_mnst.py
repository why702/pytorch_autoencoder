import torchimport torchvisionfrom torch import nnfrom torch.autograd import Variablefrom torch.utils.data import DataLoaderimport torch.utils.data as Datafrom torch.autograd import Functionfrom torch import autogradfrom torchvision import transformsimport torchvision.transforms.functional as TFfrom torchvision.utils import save_imagefrom torchvision.datasets import MNISTimport osimport utilimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport randomimport cv2from numpy.fft import rfft2, irfft2import mathuse_cuda = torch.cuda.is_available()device = torch.device('cuda:0' if use_cuda else 'cpu')# device = 'cpu'if not os.path.exists('./dc_img'):    os.mkdir('./dc_img')num_epochs = 1000batch_size = 128learning_rate = 1e-3width = 200height = 200input_folder = "D:\\git\\20201205_ET713_3PG_A52_5G_Chamber_3DB\\1206_A52_5G_25_P\\19081902\\5"output_folder = input_folder + "_AE"class FaceLandmarksDataset(Data.Dataset):    """Face Landmarks dataset."""    def __init__(self, root_dir, csv_file, transform=None):        """        Args:            csv_file (string): Path to the csv file with annotations.            root_dir (string): Directory with all the images.            transform (callable, optional): Optional transform to be applied                on a sample.        """        util.read_bins_toCSV(root_dir, csv_file, width, height, True)        self.landmarks_frame = pd.read_csv(csv_file)        self.size = self.landmarks_frame.shape[0]        self.root_dir = root_dir    def __len__(self):        return len(self.landmarks_frame)    def transform(self, image, label):        # # Resize        # resize = transforms.Resize(size=(520, 520))        # image = resize(image)        # label = resize(label)        #        # # Random crop        # i, j, h, w = transforms.RandomCrop.get_params(        #     image, output_size=(512, 512))        # image = TF.crop(image, i, j, h, w)        # label = TF.crop(label, i, j, h, w)        image = TF.to_pil_image(image)        label = TF.to_pil_image(label)        # Random horizontal flipping        if random.random() > 0.5:            image = TF.hflip(image)            label = TF.hflip(label)        # Random vertical flipping        if random.random() > 0.5:            image = TF.vflip(image)            label = TF.vflip(label)        # Transform to tensor        image = TF.to_tensor(image)        label = TF.to_tensor(label)        # image = TF.normalize(image, mean=(0,), std=(1,))        # label = TF.normalize(label, mean=(0,), std=(1,))        return image, label    def __getitem__(self, idx, trans = True):        if torch.is_tensor(idx):            idx = idx.tolist()        image = util.read_bin(self.landmarks_frame.iloc[idx, 0], (200, 200), True)        bk = util.read_bin(self.landmarks_frame.iloc[idx, 1], (200, 200), True)        ipp = util.read_8bit_bin(self.landmarks_frame.iloc[idx, 2], (200, 200), True)        diff = util.subtract(image, bk)        diff = ((diff - np.mean(diff)) / np.std(diff)).astype('float32')        ipp = ((ipp - np.mean(ipp)) / np.std(ipp)).astype('float32')        if trans:            diff, ipp = self.transform(diff, ipp)        return diff, ipp    def get_img_path(self, idx):        if torch.is_tensor(idx):            idx = idx.tolist()        image = util.read_bin(self.landmarks_frame.iloc[idx, 0], (200, 200), True)        bk = util.read_bin(self.landmarks_frame.iloc[idx, 1], (200, 200), True)        ipp_path = self.landmarks_frame.iloc[idx, 2]        diff = util.subtract(image, bk)        diff = ((diff - np.mean(diff)) / np.std(diff)).astype('float32')        diff = TF.to_tensor(diff)        diff.unsqueeze_(0)        return diff, ipp_pathdataset = FaceLandmarksDataset(root_dir=input_folder,  csv_file='list.csv')dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)class autoencoder(nn.Module):    def __init__(self):        super(autoencoder, self).__init__()        self.encoder = nn.Sequential(         # input shape (1, 200, 200)            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=64, out_channels=200, kernel_size=5, stride=1, padding=2),            nn.ELU(),        )        self.decoder = nn.Sequential(            nn.Conv2d(in_channels=200, out_channels=64, kernel_size=5, stride=1, padding=2),            nn.ELU(),            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),            nn.ELU(),            nn.Upsample(scale_factor=(2, 2), mode="bilinear", align_corners=True),            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),            nn.ELU(),        )    def forward(self, x):        x = self.encoder(x)        x = self.decoder(x)        return x# loss from FFTdef LPF_Butterworth(width, height, kRadius, kOrder):    fltDst = np.empty([height, width])    cx = width / 2    cy = height / 2    for row in range(height):        for col in range(width):            kDistance = math.sqrt((col - cx) ** 2 + (row - cy) ** 2)            fltDst[row][col] = 1 / (1 + pow((kDistance / kRadius),                                            (2 * kOrder)))    return fltDstdef HPF_Butterworth(width, height, kRadius, kOrder):    fltDst = np.empty([height, width])    cx = width / 2    cy = height / 2    for row in range(height):        for col in range(width):            kDistance = math.sqrt((col - cx) ** 2 + (row - cy) ** 2)            fltDst[row][col] = 1 - 1 / (1 + pow((kDistance / kRadius),                                                (2 * kOrder)))    return fltDstinch2mm = 25.4m_nDPI = 508szImage = (min(width, height) * inch2mm) / m_nDPIfcUp = int(szImage * 3 / 2)  # newborns babyfcLow = int(szImage * 10 / 2)  # grown-upsfltNoiseLow = LPF_Butterworth(width, height, fcLow, 4).astype(np.complex)fltNoiseHigh = HPF_Butterworth(width, height, fcUp, 4)class BadFFTFunction(Function):    @staticmethod    def forward(ctx, input):        numpy_input = input.detach().numpy()        result = abs(rfft2(numpy_input))        return input.new(result)    @staticmethod    def backward(ctx, grad_output):        numpy_go = grad_output.numpy()        result = irfft2(numpy_go)        return grad_output.new(result)def incorrect_fft(input):    return BadFFTFunction.apply(input)class FFT_NOISE(Function):    @staticmethod    def forward(ctx, input):        numpy_input = input.cpu().detach().numpy()        # split signal / noise by fft        f = np.fft.fft2(numpy_input)        f = np.fft.fftshift(f)        f_l = f * fltNoiseLow        f_l = np.fft.ifftshift(f_l)        f_l = np.fft.ifft2(f_l)        img_l = np.real(f_l)        f_h = f * fltNoiseHigh        f_h = np.fft.ifftshift(f_h)        f_h = np.fft.ifft2(f_h)        img_h = np.real(f_h)        img_s = numpy_input - img_l - img_h        img_n = numpy_input - img_s        # debug = cv2.normalize(img_s[0][0], None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)        # cv2.imshow("", debug)        # cv2.waitKey()        # crop        offset_height = int(height / 4)        offset_width = int(width / 4)        target_height = int(height / 2)        target_width = int(width / 2)        # crop_h = img_h[:, :, offset_height:offset_height + target_height, offset_width:offset_width + target_width]        crop_n = img_n[:, :, offset_height:offset_height + target_height, offset_width:offset_width + target_width]        # crop_s = img_s[:, :, offset_height:offset_height + target_height, offset_width:offset_width + target_width]        # get SNR        # img_signal_max = np.amax(crop_s, axis=(1, 2, 3))        # img_signal_min = np.amin(crop_s, axis=(1, 2, 3))        img_noise_std = np.std(crop_n, axis=(1, 2, 3))        # img_high_noise_std = np.std(crop_h, axis=(1, 2, 3))        # img_signal_diff = img_signal_max - img_signal_min        img_noise_std_mean = np.amin(img_noise_std)        # snrSignalHigh = img_signal_diff / img_high_noise_std        # snrSignalAll = img_signal_diff / img_noise_std        # return input.new(img_noise_std_mean)        # return torch.as_tensor(np.array(img_noise_std_mean).astype('float'))        # return torch.tensor(img_noise_std_mean)        out = torch.tensor(100 / img_noise_std)        ctx.save_for_backward(out)        return out    @staticmethod    def backward(ctx, grad_output):        out = ctx.saved_tensors        out = torch.tensor([item.cpu().detach().numpy() for item in out]).cuda()        grad_input = grad_output.clone()        out_ = torch.mul(grad_input, out)        # out_ = grad_input * out        return grad_inputclass DRY_SCORE(Function):    @staticmethod    def forward(ctx, input):        with torch.no_grad():            numpy_input = input.detach().numpy()            offset_height = int(height / 4)            offset_width = int(width / 4)            target_height = int(height / 2)            target_width = int(width / 2)            crop = numpy_input[:, :, offset_height:offset_height + target_height, offset_width:offset_width + target_width]            maxCorners = 1000            qualityLevel = 0.1            minDistance = 0.1            corner_list = []            for i in range(crop.shape[0]):                corners = cv2.goodFeaturesToTrack(crop[i, 0, :, :], maxCorners=maxCorners, qualityLevel=qualityLevel,                                                  minDistance=minDistance)                if corners is not None:                    corner_list.append(corners.size)                else:                    corner_list.append(0)        return input.new(np.array(corner_list))        # return torch.as_tensor(np.array(corner_list))    @staticmethod    def backward(ctx, grad_output):        return grad_outputclass MSE_SCORE(Function):    @staticmethod    def forward(ctx, input, label):        numpy_input = input.detach().numpy()        numpy_label = label.detach().numpy()        diff = (numpy_label - numpy_input)**2        # mean = np.mean(diff, axis=(1,2,3))        # mse = np.sum(mean)        mse = np.sum(diff)        ctx.save_for_backward(input, label)        return torch.tensor(mse)    @staticmethod    def backward(ctx, grad_output):        grad_output = grad_output.detach().numpy()        input, label = ctx.saved_tensors        numpy_input = input.detach().numpy()        numpy_label = label.detach().numpy()        grad = -2 * np.sum(np.mean((numpy_label - numpy_input), axis=(1,2,3))) * np.ones(input.shape) * grad_output        return torch.tensor(grad), Nonemodel = autoencoder().to(device=device)criterion = nn.MSELoss(reduce=True, size_average=False)weighting_file = './model/conv_autoencoder.pth'if os.path.exists(weighting_file):    model.load_state_dict(torch.load(weighting_file))    model.eval()    model.to(device)def criterion_fft(input):    return FFT_NOISE.apply(input)def criterion_dry(input):    return DRY_SCORE.apply(input)def criterion_mse(input, label):    return MSE_SCORE.apply(input, label)optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,                             weight_decay=1e-5)# # and a learning rate scheduler# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,#                                                    step_size=1000,#                                                    gamma=0.01)N_TEST_IMG = 5f, a = plt.subplots(3, N_TEST_IMG, figsize=(10, 6))plt.ion()   # continuously plotfor epoch in range(num_epochs):    total_loss = 0    total_fft_loss = 0    total_dry_loss = 0    for step, (img, label) in enumerate(dataloader):        img = Variable(img).to(device=device)        label = Variable(label).to(device=device)        # ===================forward=====================        output = model(img)        loss = criterion(output, label)        print(loss.grad_fn)  # MSELoss        fft_input = incorrect_fft(output.cpu()).to(device=device)        target = torch.ones_like(fft_input) * 100        fft_loss = criterion(fft_input, target)        # fft_loss = criterion_fft(output.cpu()).to(device=device)        print(fft_loss.grad_fn)        # dry_loss = criterion_dry(output.cpu()).to(device=device)        # mse_loss = criterion_mse(output.cpu(), label.cpu()).to(device=device)        total_loss += loss.data        total_fft_loss += fft_loss        # total_dry_loss += dry_loss        # ===================backward====================        optimizer.zero_grad()        # backprop and update        # fft_loss.requires_grad = True        fft_loss.sum().backward()        # fft_loss.backward(torch.tensor(1, dtype=torch.float).to(device=device))        # fft_loss.backward(torch.tensor(1.0).to(device=device) for _ in range(len(img)))        # loss_seq = [mse_loss]        # grad_seq = [torch.tensor(1.0).to(device=device) for _ in range(len(loss_seq))]        # torch.autograd.backward(loss_seq, grad_seq)        # loss.backward()        # all_loss.backward()        optimizer.step()        print([x.grad for x in model.parameters()])        if epoch % 10 == 0 and step == 0:            # plotting decoded image            for i in range(N_TEST_IMG):                a[0][i].clear()                a[0][i].imshow(np.reshape(img.data.cpu().numpy()[i], (200, 200)), cmap='gray')                a[1][i].clear()                a[1][i].imshow(np.reshape(label.data.cpu().numpy()[i], (200, 200)), cmap='gray')                a[2][i].clear()                a[2][i].imshow(np.reshape(output.data.cpu().numpy()[i], (200, 200)), cmap='gray')                pass            plt.draw()            plt.pause(0.05)            plt.savefig('./dc_img/image_{}.png'.format(epoch))    # ===================log========================    print('epoch [{}/{}], loss:{:.4f}, lr={}, fft loss:{:.6f}, dry loss:{:.6f}'          .format(epoch+1, num_epochs, total_loss, optimizer.param_groups[0]['lr'], total_fft_loss, total_dry_loss))    if epoch % 100 == 0 and epoch > 0:        torch.save(model.state_dict(), './model/conv_autoencoder_{}.pth'.format(epoch))        print('save ./model/conv_autoencoder_{}.pth'.format(epoch))